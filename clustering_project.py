# -*- coding: utf-8 -*-
"""Clustering Project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1th5Sw5XQkL9cD-4B3o3JiYZGVejN4lY5

# GUC Clustering Project

**Objective:** 
The objective of this project teach students how to apply clustering to real data sets

The projects aims to teach student: 
* Which clustering approach to use
* Compare between Kmeans, Hierarchal, DBScan, and Gaussian Mixtures  
* How to tune the parameters of each data approach
* What is the effect of different distance functions (optional) 
* How to evaluate clustering approachs 
* How to display the output
* What is the effect of normalizing the data 

Students in this project will use ready-made functions from Sklearn, plotnine, numpy and pandas
"""

# if plotnine is not installed in Jupter then use the following command to install it 
!pip install plotnine

"""Running this project require the following imports """

# Commented out IPython magic to ensure Python compatibility.
import itertools
import warnings
warnings.filterwarnings('ignore')
import seaborn as sns 
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
import sklearn.preprocessing as prep
from sklearn.datasets import make_blobs
from plotnine import *   
from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import NearestNeighbors
from sklearn.cluster import AgglomerativeClustering
from scipy.cluster.hierarchy import dendrogram, linkage
from sklearn.cluster import DBSCAN
from sklearn.cluster import KMeans
from sklearn.mixture import GaussianMixture
from sklearn.metrics import silhouette_score
from scipy.stats import multivariate_normal

# %matplotlib inline

# helper function that allows us to display data in 2 dimensions an highlights the clusters
def display_cluster(X,km=[],num_clusters=0):
    color = ['Red', 'Blue', 'Green', 'Yellow', 'Purple', 'Orange', 'Pink', 'Brown', 'White', 'Gray', 'Cyan', 'Magenta', 'Turquoise', 'Olive', 'Maroon', 'Navy', 'Teal', 'Lavender', 'Salmon', 'Gold', 'Silver', 'Lime', 'Tan', 'Beige', 'Indigo', 'Violet', 'Crimson', 'Fuchsia', 'Black']
    alpha = 0.5  #color obaque
    s = 20
    if num_clusters == 0:
        plt.scatter(X[:,0],X[:,1],c = color[0],alpha = alpha,s = s)
    else:
        for i in range(-1,num_clusters,1):
            plt.scatter(X[km.labels_==i,0],X[km.labels_==i,1],c = color[i%29],alpha = alpha,s=s)
#             plt.scatter(km.cluster_centers_[i][0],km.cluster_centers_[i][1],c = color[i%7], marker = 'x', s = 100)

"""## Multi Blob Data Set 
* The Data Set generated below has 6 cluster with varying number of users and varing densities
* Cluster the data set below using 


"""

plt.rcParams['figure.figsize'] = [8,8]
sns.set_style("whitegrid")
sns.set_context("talk")

n_bins = 6  
centers = [(-3, -3), (0, 0), (5,2.5),(-1, 4), (4, 6), (9,7)]
Multi_blob_Data, y = make_blobs(n_samples=[100,150, 300, 400,300, 200], n_features=2, cluster_std=[1.3,0.6, 1.2, 1.7,0.9,1.7],
                  centers=centers, shuffle=False, random_state=42)
display_cluster(Multi_blob_Data)

"""### Kmeans 
* Use Kmeans with different values of K to cluster the above data 
* Display the outcome of each value of K 
* Plot distortion function versus K and choose the approriate value of k 
* Plot the silhouette_score versus K and use it to choose the best K 
* Store the silhouette_score for the best K for later comparison with other clustering techniques. 
"""

df_copy = np.NaN

def GUC_Kmean(Data_points, Number_of_Clusters):
    global df_copy
    
    df_copy = Data_points.copy()
    
    kmeans = KMeans(n_clusters=Number_of_Clusters).fit(Data_points)
    labels = kmeans.labels_
    df_copy['cluster'] = labels
    
    return kmeans

Kmeans_output = []

for i in range(2,11):
    km = GUC_Kmean(pd.DataFrame(Multi_blob_Data), i)
    
    plt.figure()
    plt.title(f'Kmeans Clustering using K equal to {i}')
    display_cluster(df_copy.to_numpy(copy=True), km, i)
    
    Kmeans_output.append(km)

distortion = []
sil_score = []

for i in range(2,11):
    sil_score.append(silhouette_score(pd.DataFrame(Multi_blob_Data), Kmeans_output[i-2].labels_))
    distortion.append(Kmeans_output[i-2].inertia_)

plt.figure()
plt.title(f'Ploting silhouette_score to determine the best K')
plt.scatter(range(2,11), sil_score)

plt.figure()
plt.title('Ploting distortion to determine the best K using Elbow method')
plt.plot(range(2,11), distortion)

"""### Hierarchal Clustering
* Use AgglomerativeClustering function to  to cluster the above data 
* In the  AgglomerativeClustering change the following parameters 
    * Affinity (use euclidean, manhattan and cosine)
    * Linkage( use average and single )
    * Distance_threshold (try different)
* For each of these trials plot the Dendograph , calculate the silhouette_score and display the resulting clusters  
* Find the set of paramters that would find result in the best silhouette_score and store this score for later comparison with other clustering techniques. 
* Record your observation 
"""

def plot_Agglomerative(aff, link, dist, model, df, cluster_num, flag):
    if flag:
        plt.figure()
        plt.title(f'Dendrogram using {aff} for Affininty, and {link} for Linkage type')
        met = 'cityblock' if aff == 'manhattan' else aff
        Z = linkage(df, method=link, metric=met)
        dendrogram(Z, p=dist)
        plt.show()
    
    plt.figure()
    plt.title(f'Hierarchal Clustering using {aff} for Affininty, and {link} for Linkage type and distance thershold = {dist}')
    display_cluster(df.to_numpy(copy=True), model, cluster_num)

for aff in ['euclidean', 'manhattan']:
    for link in ['average', 'single']:
        model = np.NaN
        if link == 'average':
            flag = True
            for dist in np.arange(2,10,2):
                model = AgglomerativeClustering(n_clusters=None, compute_full_tree=True, linkage=link, affinity=aff, distance_threshold=dist)
                model.fit(pd.DataFrame(Multi_blob_Data))
                plot_Agglomerative(aff, link, float(dist), model, pd.DataFrame(Multi_blob_Data), len(np.unique(model.labels_)), flag)
                flag = False
                score = silhouette_score(pd.DataFrame(Multi_blob_Data), model.labels_)
                plt.figure()
                plt.scatter(Multi_blob_Data[:, 0], Multi_blob_Data[:, 1], c=model.labels_, cmap='viridis')
                plt.title(f"Agglomerative Clustering\n Silhouette Score: {score:.2f}")
                plt.xlabel('X')
                plt.ylabel('Y')
                plt.show()
            
        elif link == 'single':
            flag = True
            for dist in np.arange(1,2,0.2):
                model = AgglomerativeClustering(n_clusters=None, compute_full_tree=True, linkage=link, affinity=aff, distance_threshold=dist)
                model.fit(pd.DataFrame(Multi_blob_Data))
                plot_Agglomerative(aff, link, float(dist), model, pd.DataFrame(Multi_blob_Data), len(np.unique(model.labels_)), flag)
                flag = False
                score = silhouette_score(pd.DataFrame(Multi_blob_Data), model.labels_)
                plt.figure()
                plt.scatter(Multi_blob_Data[:, 0], Multi_blob_Data[:, 1], c=model.labels_, cmap='viridis')
                plt.title(f"Agglomerative Clustering\n Silhouette Score: {score:.2f}")
                plt.xlabel('X')
                plt.ylabel('Y')
                plt.show()
                
for aff in ['cosine']:
    for link in ['average', 'single']:
        model = np.NaN
        if link == 'average':
            flag = True
            for dist in np.arange(0.2,1.6,0.4):
                model = AgglomerativeClustering(n_clusters=None, compute_full_tree=True, linkage=link, affinity=aff, distance_threshold=dist)
                model.fit(pd.DataFrame(Multi_blob_Data))
                plot_Agglomerative(aff, link, float(dist), model, pd.DataFrame(Multi_blob_Data), len(np.unique(model.labels_)), flag)
                flag = False
                score = silhouette_score(pd.DataFrame(Multi_blob_Data), model.labels_)
                plt.figure()
                plt.scatter(Multi_blob_Data[:, 0], Multi_blob_Data[:, 1], c=model.labels_, cmap='viridis')
                plt.title(f"Agglomerative Clustering\n Silhouette Score: {score:.2f}")
                plt.xlabel('X')
                plt.ylabel('Y')
                plt.show()
            
        elif link == 'single':
            flag = True
            for dist in np.arange(0.003,0.01,0.003):
                model = AgglomerativeClustering(n_clusters=None, compute_full_tree=True, linkage=link, affinity=aff, distance_threshold=dist)
                model.fit(pd.DataFrame(Multi_blob_Data))
                plot_Agglomerative(aff, link, float(dist), model, pd.DataFrame(Multi_blob_Data), len(np.unique(model.labels_)), flag)
                flag = False
                score = silhouette_score(pd.DataFrame(Multi_blob_Data), model.labels_)
                plt.figure()
                plt.scatter(Multi_blob_Data[:, 0], Multi_blob_Data[:, 1], c=model.labels_, cmap='viridis')
                plt.title(f"Agglomerative Clustering\n Silhouette Score: {score:.2f}")
                plt.xlabel('X')
                plt.ylabel('Y')
                plt.show()

"""### DBScan
* Use DBScan function to  to cluster the above data 
* In the  DBscan change the following parameters 
    * EPS (from 0.1 to 3)
    * Min_samples (from 5 to 25)
* Plot the silhouette_score versus the variation in the EPS and the min_samples
* Plot the resulting Clusters in this case 
* Find the set of paramters that would find result in the best silhouette_score and store this score for later comparison with other clustering techniques. 
* Record your observations and comments 
"""

EPS_range = np.arange(0.1, 0.9, 0.1)
min_samples_range = np.arange(5, 25, 5)

for EPS in EPS_range:
    for min_sample in min_samples_range:
        dbscan = DBSCAN(eps=EPS, min_samples=min_sample)
        dbscan.fit(pd.DataFrame(Multi_blob_Data))
        
        plt.figure()
        plt.title(f'DBScan with EPS = {EPS} and min_samples = {min_sample}')
        display_cluster(Multi_blob_Data, dbscan, 20)

silhouette_scores = np.zeros((len(EPS_range), len(min_samples_range)))

for i, eps in enumerate(EPS_range):
    for j, min_samples in enumerate(min_samples_range):
        dbscan = DBSCAN(eps=eps, min_samples=min_samples)
        labels = dbscan.fit_predict(pd.DataFrame(Multi_blob_Data))
        if len(set(labels)) > 1:  # Silhouette score is undefined for a single cluster
            silhouette_scores[i, j] = silhouette_score(pd.DataFrame(Multi_blob_Data), labels)

# Plot results
plt.scatter(EPS_range.repeat(len(min_samples_range)), np.tile(list(min_samples_range), len(EPS_range)), c=silhouette_scores.ravel())
plt.xlabel('EPS')
plt.ylabel('Min Samples')
plt.title('Silhouette scores for DBSCAN models')
plt.colorbar()
plt.show()

"""### Gaussian Mixture
* Use GaussianMixture function to cluster the above data 
* In GMM change the covariance_type and check the difference in the resulting proabability fit 
* Use a 2D contour plot to plot the resulting distribution (the components of the GMM) as well as the total Gaussian mixture 
"""

for type in ['spherical', 'diag', 'tied', 'full']:
    # Fit GMM to the data
    n_components = 6
    gmm = GaussianMixture(n_components=n_components, covariance_type=type, random_state=0)
    gmm.fit(pd.DataFrame(Multi_blob_Data))

    # Define the grid for the contour plot
    x_min, x_max = -10, 15
    y_min, y_max = -10, 15
    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100), np.linspace(y_min, y_max, 100))
    Z = -gmm.score_samples(np.c_[xx.ravel(), yy.ravel()])
    Z = Z.reshape(xx.shape)

    # Create the contour plot
    plt.contourf(xx, yy, Z, cmap=plt.cm.Blues_r)
    plt.scatter(Multi_blob_Data[:, 0], Multi_blob_Data[:, 1], s=5, color='black')
    plt.title(f'Gaussian Mixture Model with covariance_type = {type}')
    plt.xlabel('Feature 1')
    plt.ylabel('Feature 2')
    plt.show()

"""## iris data set 
The iris data set is test data set that is part of the Sklearn module 
which contains 150 records each with 4 features. All the features are represented by real numbers 

The data represents three classes 

"""

import json

from sklearn.datasets import load_iris
iris_data = load_iris()
iris_data.target[[10, 25, 50]]
#array([0, 0, 1])
list(iris_data.target_names)
['setosa', 'versicolor', 'virginica']

df_iris = pd.DataFrame(data= np.c_[iris_data['data'], iris_data['target']],columns= iris_data['feature_names'] + ['target'])

Kmeans_output = []

for i in range(2,11):
    km = GUC_Kmean(df_iris, i)
    
    plt.figure()
    plt.title(f'Kmeans Clustering using K equal to {i}')
    display_cluster(df_copy[['sepal length (cm)', 'target']].to_numpy(copy=True), km, i)
    
    Kmeans_output.append(km)

distortion = []
sil_score = []

for i in range(2,11):
    sil_score.append(silhouette_score(df_iris, Kmeans_output[i-2].labels_))
    distortion.append(Kmeans_output[i-2].inertia_)

plt.figure()
plt.title(f'Ploting silhouette_score to determine the best K')
plt.scatter(range(2,11), sil_score)

plt.figure()
plt.title('Ploting distortion to determine the best K using Elbow method')
plt.plot(range(2,11), distortion)

for aff in ['euclidean', 'manhattan']:
    for link in ['average', 'single']:
        model = np.NaN
        if link == 'average':
            flag = True
            for dist in np.arange(0.5,2.5,0.5):
                model = AgglomerativeClustering(n_clusters=None, compute_full_tree=True, linkage=link, affinity=aff, distance_threshold=dist)
                model.fit(df_iris)
                plot_Agglomerative(aff, link, float(dist), model, df_iris[['sepal length (cm)', 'target']], len(np.unique(model.labels_)), flag)
                flag = False
                score = silhouette_score(df_iris, model.labels_)
                plt.figure()
                plt.scatter(df_iris.to_numpy(copy=True)[:, 0], df_iris.to_numpy(copy=True)[:, -1], c=model.labels_, cmap='viridis')
                plt.title(f"Agglomerative Clustering\n Silhouette Score: {score:.2f}")
                plt.xlabel('X')
                plt.ylabel('Y')
                plt.show()
            
        elif link == 'single':
            flag = True
            for dist in np.arange(0.2,1,0.2):
                model = AgglomerativeClustering(n_clusters=None, compute_full_tree=True, linkage=link, affinity=aff, distance_threshold=dist)
                model.fit(df_iris)
                plot_Agglomerative(aff, link, float(dist), model, df_iris[['sepal length (cm)', 'target']], len(np.unique(model.labels_)), flag)
                flag = False
                score = silhouette_score(df_iris, model.labels_)
                plt.figure()
                plt.scatter(df_iris.to_numpy(copy=True)[:, 0], df_iris.to_numpy(copy=True)[:, -1], c=model.labels_, cmap='viridis')
                plt.title(f"Agglomerative Clustering\n Silhouette Score: {score:.2f}")
                plt.xlabel('X')
                plt.ylabel('Y')
                plt.show()

for aff in ['cosine']:
    for link in ['average', 'single']:
        model = np.NaN
        if link == 'average':
            flag = True
            for dist in np.arange(0.002,0.015,0.005):
                model = AgglomerativeClustering(n_clusters=None, compute_full_tree=True, linkage=link, affinity=aff, distance_threshold=dist)
                model.fit(df_iris)
                plot_Agglomerative(aff, link, float(dist), model, df_iris[['sepal length (cm)', 'target']], len(np.unique(model.labels_)), flag)
                flag = False
                score = silhouette_score(df_iris, model.labels_)
                plt.figure()
                plt.scatter(df_iris.to_numpy(copy=True)[:, 0], df_iris.to_numpy(copy=True)[:, -1], c=model.labels_, cmap='viridis')
                plt.title(f"Agglomerative Clustering\n Silhouette Score: {score:.2f}")
                plt.xlabel('X')
                plt.ylabel('Y')
                plt.show()
            
        elif link == 'single':
            flag = True
            for dist in np.arange(0.001,0.01,0.003):
                model = AgglomerativeClustering(n_clusters=None, compute_full_tree=True, linkage=link, affinity=aff, distance_threshold=dist)
                model.fit(df_iris)
                plot_Agglomerative(aff, link, float(dist), model, df_iris[['sepal length (cm)', 'target']], len(np.unique(model.labels_)), flag)
                flag = False
                score = silhouette_score(df_iris, model.labels_)
                plt.figure()
                plt.scatter(df_iris.to_numpy(copy=True)[:, 0], df_iris.to_numpy(copy=True)[:, -1], c=model.labels_, cmap='viridis')
                plt.title(f"Agglomerative Clustering\n Silhouette Score: {score:.2f}")
                plt.xlabel('X')
                plt.ylabel('Y')
                plt.show()

EPS_range = np.arange(0.1, 0.9, 0.1)
min_smaples_range = np.arange(5, 25, 5)

for EPS in EPS_range:
    for min_sample in min_smaples_range:
        dbscan = DBSCAN(eps=EPS, min_samples=min_sample)
        dbscan.fit(df_iris)
        
        plt.figure()
        plt.title(f'DBScan with EPS = {EPS} and min_samples = {min_sample}')
        display_cluster(df_iris[['sepal length (cm)', 'target']].to_numpy(copy=True), dbscan, 20)

silhouette_scores = np.zeros((len(EPS_range), len(min_smaples_range)))

for i, eps in enumerate(EPS_range):
    for j, min_samples in enumerate(min_smaples_range):
        dbscan = DBSCAN(eps=eps, min_samples=min_samples)
        labels = dbscan.fit_predict(df_iris)
        if len(set(labels)) > 1:  # Silhouette score is undefined for a single cluster
            silhouette_scores[i, j] = silhouette_score(df_iris, labels)

# Plot results
plt.scatter(EPS_range.repeat(len(min_samples_range)), np.tile(list(min_samples_range), len(EPS_range)), c=silhouette_scores.ravel())
plt.xlabel('EPS')
plt.ylabel('Min Samples')
plt.title('Silhouette scores for DBSCAN models')
plt.colorbar()
plt.show()

gmm = GaussianMixture(n_components=2, covariance_type='full')
gmm.fit(df_iris)

# create a grid of points to evaluate the PDFs
x_min, x_max = df_iris.to_numpy(copy=True)[:, 0].min() - 1, df_iris.to_numpy(copy=True)[:, 0].max() + 1
y_min, y_max = df_iris.to_numpy(copy=True)[:, -1].min() - 1, df_iris.to_numpy(copy=True)[:, -1].max() + 1
xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100), np.linspace(y_min, y_max, 100))

# calculate the PDF of each component and the total mixture
Z = np.zeros((xx.shape[0], xx.shape[1]))
for i in range(gmm.n_components):
    # calculate the PDF of the i-th component
    mu = gmm.means_[i][:2]  # take the first two dimensions for plotting
    sigma = gmm.covariances_[i][:2, :2]  # take the first two dimensions for plotting
    rv = multivariate_normal(mu, sigma)
    Z += gmm.weights_[i] * rv.pdf(np.dstack((xx, yy)))

# plot the contours
plt.figure(figsize=(8, 6))
plt.contour(xx, yy, Z, levels=10, linewidths=1, colors='k', alpha=0.5)
plt.title('Gaussian Mixture Model with covariance_type = full')
plt.xlabel('sepal length (cm)')
plt.ylabel('Target')
plt.show()

"""# Normalize"""

scaler = StandardScaler()
df_iris_scaled = pd.DataFrame(scaler.fit_transform(df_iris))
df_iris_scaled.columns = ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)','petal width (cm)', 'target']

Kmeans_output = []

for i in range(2,11):
    km = GUC_Kmean(df_iris_scaled, i)
    
    plt.figure()
    plt.title(f'Kmeans Clustering using K equal to {i}')
    display_cluster(df_copy[['sepal length (cm)', 'target']].to_numpy(copy=True), km, i)
    
    Kmeans_output.append(km)

distortion = []
sil_score = []

for i in range(2,11):
    sil_score.append(silhouette_score(df_iris_scaled, Kmeans_output[i-2].labels_))
    distortion.append(Kmeans_output[i-2].inertia_)

plt.figure()
plt.title(f'Ploting silhouette_score to determine the best K')
plt.scatter(range(2,11), sil_score)

plt.figure()
plt.title('Ploting distortion to determine the best K using Elbow method')
plt.plot(range(2,11), distortion)

for aff in ['euclidean', 'manhattan']:
    for link in ['average', 'single']:
        model = np.NaN
        if link == 'average':
            flag = True
            for dist in np.arange(0.5,2.5,0.5):
                model = AgglomerativeClustering(n_clusters=None, compute_full_tree=True, linkage=link, affinity=aff, distance_threshold=dist)
                model.fit(df_iris_scaled)
                plot_Agglomerative(aff, link, float(dist), model, df_iris_scaled[['sepal length (cm)', 'target']], len(np.unique(model.labels_)), flag)
                flag = False
                score = silhouette_score(df_iris_scaled, model.labels_)
                plt.figure()
                plt.scatter(df_iris_scaled.to_numpy(copy=True)[:, 0], df_iris_scaled.to_numpy(copy=True)[:, -1], c=model.labels_, cmap='viridis')
                plt.title(f"Agglomerative Clustering\n Silhouette Score: {score:.2f}")
                plt.xlabel('X')
                plt.ylabel('Y')
                plt.show()
            
        elif link == 'single':
            flag = True
            scores = []
            for dist in np.arange(0.2,1.2,0.2):
                model = AgglomerativeClustering(n_clusters=None, compute_full_tree=True, linkage=link, affinity=aff, distance_threshold=dist)
                model.fit(df_iris_scaled)
                plot_Agglomerative(aff, link, float(dist), model, df_iris_scaled[['sepal length (cm)', 'target']], len(np.unique(model.labels_)), flag)
                flag = False
                score = silhouette_score(df_iris_scaled, model.labels_)
                plt.figure()
                plt.scatter(df_iris_scaled.to_numpy(copy=True)[:, 0], df_iris_scaled.to_numpy(copy=True)[:, -1], c=model.labels_, cmap='viridis')
                plt.title(f"Agglomerative Clustering\n Silhouette Score: {score:.2f}")
                plt.xlabel('X')
                plt.ylabel('Y')
                plt.show()
            
for aff in ['cosine']:
    for link in ['average', 'single']:
        model = np.NaN
        if link == 'average':
            flag = True
            for dist in np.arange(0.25,1.0,0.25):
                model = AgglomerativeClustering(n_clusters=None, compute_full_tree=True, linkage=link, affinity=aff, distance_threshold=dist)
                model.fit(df_iris_scaled)
                plot_Agglomerative(aff, link, float(dist), model, df_iris_scaled[['sepal length (cm)', 'target']], len(np.unique(model.labels_)), flag)
                flag = False
                score = silhouette_score(df_iris_scaled, model.labels_)
                plt.figure()
                plt.scatter(df_iris_scaled.to_numpy(copy=True)[:, 0], df_iris_scaled.to_numpy(copy=True)[:, -1], c=model.labels_, cmap='viridis')
                plt.title(f"Agglomerative Clustering\n Silhouette Score: {score:.2f}")
                plt.xlabel('X')
                plt.ylabel('Y')
                plt.show()
            
        elif link == 'single':
            flag = True
            for dist in np.arange(0.025,0.16,0.025):
                model = AgglomerativeClustering(n_clusters=None, compute_full_tree=True, linkage=link, affinity=aff, distance_threshold=dist)
                model.fit(df_iris_scaled)
                plot_Agglomerative(aff, link, float(dist), model, df_iris_scaled[['sepal length (cm)', 'target']], len(np.unique(model.labels_)), flag)
                flag = False
                score = silhouette_score(df_iris_scaled, model.labels_)
                plt.figure()
                plt.scatter(df_iris_scaled.to_numpy(copy=True)[:, 0], df_iris_scaled.to_numpy(copy=True)[:, -1], c=model.labels_, cmap='viridis')
                plt.title(f"Agglomerative Clustering\n Silhouette Score: {score:.2f}")
                plt.xlabel('X')
                plt.ylabel('Y')
                plt.show()

EPS_range = np.arange(0.1, 0.9, 0.1)
min_smaples_range = np.arange(5, 25, 5)

for EPS in EPS_range:
    for min_sample in min_smaples_range:
        dbscan = DBSCAN(eps=EPS, min_samples=min_sample)
        dbscan.fit(df_iris_scaled)
        
        plt.figure()
        plt.title(f'DBScan with EPS = {EPS} and min_samples = {min_sample}')
        display_cluster(df_iris_scaled[['sepal length (cm)', 'target']].to_numpy(copy=True), dbscan, 20)

silhouette_scores = np.zeros((len(EPS_range), len(min_smaples_range)))

for i, eps in enumerate(EPS_range):
    for j, min_samples in enumerate(min_smaples_range):
        dbscan = DBSCAN(eps=eps, min_samples=min_samples)
        labels = dbscan.fit_predict(df_iris_scaled)
        if len(set(labels)) > 1:  # Silhouette score is undefined for a single cluster
            silhouette_scores[i, j] = silhouette_score(df_iris_scaled, labels)

# Plot results
plt.scatter(EPS_range.repeat(len(min_samples_range)), np.tile(list(min_samples_range), len(EPS_range)), c=silhouette_scores.ravel())
plt.xlabel('EPS')
plt.ylabel('Min Samples')
plt.title('Silhouette scores for DBSCAN models')
plt.colorbar()
plt.show()

# Standardize data
scaler = StandardScaler()
X = scaler.fit_transform(df_iris)

# fit a Gaussian Mixture Model with 3 components
gmm = GaussianMixture(n_components=2, covariance_type='full')
gmm.fit(X)

# create a grid of points to evaluate the PDFs
x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
y_min, y_max = X[:, -1].min() - 1, X[:, -1].max() + 1
xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100), np.linspace(y_min, y_max, 100))

# calculate the PDF of each component and the total mixture
Z = np.zeros((xx.shape[0], xx.shape[1]))
for i in range(gmm.n_components):
    # calculate the PDF of the i-th component
    mu = gmm.means_[i][:2]  # take the first two dimensions for plotting
    sigma = gmm.covariances_[i][:2, :2]  # take the first two dimensions for plotting
    rv = multivariate_normal(mu, sigma)
    Z += gmm.weights_[i] * rv.pdf(np.dstack((xx, yy)))

# plot the contours
plt.figure(figsize=(8, 6))
plt.contour(xx, yy, Z, levels=10, linewidths=1, colors='k', alpha=0.5)
plt.title('Gaussian Mixture Model with covariance_type = full')
plt.xlabel('sepal length (cm)')
plt.ylabel('Target')
plt.show()

"""* Repeat all the above clustering approaches and steps on the above data 
* Normalize the data then repeat all the above steps 
* Compare between the different clustering approaches

## Customer dataset
Repeat all the above on the customer data set
"""

df_customers = pd.read_csv(r'E:\Intranet\Semester 10\(NETW 1013) Machine Learning\Assignments\Assignment 1\Assignment 1\Customer data.csv')
df_customers.drop(labels=['ID'],axis='columns',inplace=True)

scaler = StandardScaler()
df_customers_scaled = pd.DataFrame(scaler.fit_transform(df_customers))
df_customers_scaled.columns = df_customers.columns

Kmeans_output = []

for i in range(2,11):
    km = GUC_Kmean(df_customers_scaled, i)
    
    plt.figure()
    plt.title(f'Kmeans Clustering using K equal to {i}')
    display_cluster(df_copy[['Age', 'Income']].to_numpy(copy=True), km, i)
    
    Kmeans_output.append(km)

distortion = []
sil_score = []

for i in range(2,11):
    sil_score.append(silhouette_score(df_customers, Kmeans_output[i-2].labels_))
    distortion.append(Kmeans_output[i-2].inertia_)

plt.figure()
plt.title(f'Ploting silhouette_score to determine the best K')
plt.scatter(range(2,11), sil_score)

plt.figure()
plt.title('Ploting distortion to determine the best K using Elbow method')
plt.plot(range(2,11), distortion)

for aff in ['euclidean', 'manhattan']:
    for link in ['average', 'single']:
        model = np.NaN
        if link == 'average':
            flag = True
            for dist in np.arange(2,4.5,0.5):
                model = AgglomerativeClustering(n_clusters=None, compute_full_tree=True, linkage=link, affinity=aff, distance_threshold=dist)
                model.fit(df_customers_scaled)
                plot_Agglomerative(aff, link, float(dist), model, df_customers_scaled[['Age', 'Income']], len(np.unique(model.labels_)), flag)
                flag = False
                score = silhouette_score(df_customers_scaled, model.labels_)
                plt.figure()
                plt.scatter(df_customers_scaled.to_numpy(copy=True)[:, 2], df_customers_scaled.to_numpy(copy=True)[:, 4], c=model.labels_, cmap='viridis')
                plt.title(f"Agglomerative Clustering\n Silhouette Score: {score:.2f}")
                plt.xlabel('X')
                plt.ylabel('Y')
                plt.show()
            
        elif link == 'single':
            flag = True 
            for dist in np.arange(0.2,1.2,0.2):
                model = AgglomerativeClustering(n_clusters=None, compute_full_tree=True, linkage=link, affinity=aff, distance_threshold=dist)
                model.fit(df_customers_scaled)
                plot_Agglomerative(aff, link, float(dist), model, df_customers_scaled[['Age', 'Income']], len(np.unique(model.labels_)), flag)
                flag = False
                score = silhouette_score(df_customers_scaled, model.labels_)
                plt.figure()
                plt.scatter(df_customers_scaled.to_numpy(copy=True)[:, 2], df_customers_scaled.to_numpy(copy=True)[:, 4], c=model.labels_, cmap='viridis')
                plt.title(f"Agglomerative Clustering\n Silhouette Score: {score:.2f}")
                plt.xlabel('X')
                plt.ylabel('Y')
                plt.show()
                
for aff in ['cosine']:
    for link in ['average', 'single']:
        model = np.NaN
        if link == 'average':
            flag = True
            for dist in np.arange(0.2,1.2,0.2):
                model = AgglomerativeClustering(n_clusters=None, compute_full_tree=True, linkage=link, affinity=aff, distance_threshold=dist)
                model.fit(df_customers_scaled)
                plot_Agglomerative(aff, link, float(dist), model, df_customers_scaled[['Age', 'Income']], len(np.unique(model.labels_)), flag)
                flag = False
                score = silhouette_score(df_customers_scaled, model.labels_)
                plt.figure()
                plt.scatter(df_customers_scaled.to_numpy(copy=True)[:, 2], df_customers_scaled.to_numpy(copy=True)[:, 4], c=model.labels_, cmap='viridis')
                plt.title(f"Agglomerative Clustering\n Silhouette Score: {score:.2f}")
                plt.xlabel('X')
                plt.ylabel('Y')
                plt.show()
            
        elif link == 'single':
            flag = True 
            for dist in np.arange(0.0002,0.0008,0.0002):
                model = AgglomerativeClustering(n_clusters=None, compute_full_tree=True, linkage=link, affinity=aff, distance_threshold=dist)
                model.fit(df_customers_scaled)
                plot_Agglomerative(aff, link, float(dist), model, df_customers_scaled[['Age', 'Income']], len(np.unique(model.labels_)), flag)
                flag = False
                score = silhouette_score(df_customers_scaled, model.labels_)
                plt.figure()
                plt.scatter(df_customers_scaled.to_numpy(copy=True)[:, 2], df_customers_scaled.to_numpy(copy=True)[:, 4], c=model.labels_, cmap='viridis')
                plt.title(f"Agglomerative Clustering\n Silhouette Score: {score:.2f}")
                plt.xlabel('X')
                plt.ylabel('Y')
                plt.show()

EPS_range = np.arange(1.9, 2.01, 0.01)
min_samples_range = np.arange(2, 8, 2)

for EPS in EPS_range:
    for min_sample in min_samples_range:
        dbscan = DBSCAN(eps=EPS, min_samples=min_sample)
        dbscan.fit(df_customers_scaled)
        
        plt.figure()
        plt.title(f'DBScan with EPS = {EPS} and min_samples = {min_sample}')
        display_cluster(df_customers_scaled[['Age', 'Income']].to_numpy(copy=True), dbscan, 20)

silhouette_scores = np.zeros((len(EPS_range), len(min_samples_range)))

for i, eps in enumerate(EPS_range):
    for j, min_samples in enumerate(min_samples_range):
        dbscan = DBSCAN(eps=eps, min_samples=min_samples)
        labels = dbscan.fit_predict(df_customers_scaled)
        if len(set(labels)) > 1:  # Silhouette score is undefined for a single cluster
            silhouette_scores[i, j] = silhouette_score(df_customers_scaled, labels)

# Plot results
plt.scatter(EPS_range.repeat(len(min_samples_range)), np.tile(list(min_samples_range), len(EPS_range)), c=silhouette_scores.ravel())
plt.xlabel('EPS')
plt.ylabel('Min Samples')
plt.title('Silhouette scores for DBSCAN models')
plt.colorbar()
plt.show()

# Standardize data
scaler = StandardScaler()
X = scaler.fit_transform(df_customers_scaled)

gmm = GaussianMixture(n_components=5, covariance_type='full')
gmm.fit(X)

# create a grid of points to evaluate the PDFs
x_min, x_max = df_customers_scaled.to_numpy(copy=True)[:, 2].min() - 1, df_customers_scaled.to_numpy(copy=True)[:, 2].max() + 1
y_min, y_max = df_customers_scaled.to_numpy(copy=True)[:, 4].min() - 1, df_customers_scaled.to_numpy(copy=True)[:, 4].max() + 1
xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100), np.linspace(y_min, y_max, 100))

# calculate the PDF of each component and the total mixture
Z = np.zeros((xx.shape[0], xx.shape[1]))
for i in range(gmm.n_components):
    # calculate the PDF of the i-th component
    mu = gmm.means_[i][:2]  # take the first two dimensions for plotting
    sigma = gmm.covariances_[i][:2, :2]  # take the first two dimensions for plotting
    rv = multivariate_normal(mu, sigma)
    Z += gmm.weights_[i] * rv.pdf(np.dstack((xx, yy)))

# plot the contours
plt.figure(figsize=(24, 16))
plt.contour(xx, yy, Z, levels=10, linewidths=1, colors='k', alpha=0.5)
plt.title('Gaussian Mixture Model with covariance_type = full')
plt.xlabel('Age')
plt.ylabel('Income')
plt.show()